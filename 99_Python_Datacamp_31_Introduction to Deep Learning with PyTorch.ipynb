{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n",
    "\n",
    "https://campus.datacamp.com/courses/introduction-to-deep-learning-with-pytorch/introduction-to-pytorch?ex=1#_=_\n",
    "    \n",
    "    \n",
    "#### 01 Introduction to PyTorch\n",
    "\n",
    "In this first chapter, we introduce basic concepts of neural networks and deep learning using PyTorch library.\n",
    "\n",
    "introduction to PyTorch, creating tensors in PyTorch, Matrix multiplication, forward propagation, forward pass, backpropagation by hand, backpropagation using PyTorch, Calculating gradiens in Pytorch, introduction to neural networks, your first neural network, your first PyTorch neural network.\n",
    "\n",
    "\n",
    "\n",
    "#### 02 Artificial Neural Networks\n",
    "\n",
    "In this second chapter, we delve deeper into Artificial Neural Networks, learning how to train them with real datasets.\n",
    "\n",
    "Activation functions, Neural networks, ReLU activation, Loss functions, Calculation Loss function by hand, calculating loss function in PyTorch, Loss function of random scores, Preparing a dataset in PyTorch, Preparing MNist DATASET, inspecting the dataloaders, training neural networks, building a neural network, training a neural network, using the network to make predictions\n",
    "\n",
    "#### 03 Convolutional Neural Networks (CNNs)\n",
    "\n",
    "In this third chapter, we introduce convolutional neural networks, learning how to train them and how to use them to make predictions\n",
    "\n",
    "Convolution operators, convolution operator - OOP way, convolution operator - functional way, pooling operators, max-pooling operator, convolutional neural networks, your first CNN - __init__ method, your first CNN - forward() method\n",
    "Training Convolutional Neural Networks, Training CNNs, Using CNNs to make predictions\n",
    "\n",
    "#### 04 Using Convolutional Neural Networks\n",
    "\n",
    "In this last chapter, we learn how to make neural networks work well in practice, using concepts like regularization, batch-normalization and transfer learning.\n",
    "\n",
    "The sequential module, sequential module  - init method, sequential module -forward() method, the problem of overfitting, validation set, detecting overfitting, regulatization techniques, L2 - regulatization, dropout, batch-normalization, transfer learning, Finetunning a CNN, torchvision module\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/what-is-pytorch-a84e4559f0e3\n",
    "\n",
    "\n",
    "#### Introduction to PyTorch\n",
    "\n",
    "###### Creating tensors in PyTorch\n",
    "Random tensors are very important in neural networks. Parameters of the neural networks typically are initialized with random weights (random tensors).\n",
    "\n",
    "Let us start practicing building tensors in PyTorch library. As you know, tensors are arrays with an arbitrary number of dimensions, corresponding to NumPy's ndarrays. You are going to create a random tensor of sizes 3 by 3 and set it to variable your_first_tensor. Then, you will need to print it. Finally, calculate its size in variable tensor_size and print its value.\n",
    "\n",
    "Import PyTorch main library.\n",
    "Create the variable your_first_tensor and set it to a random torch tensor of size 3 by 3.\n",
    "Calculate its shape (dimension sizes) and set it to variable tensor_size.\n",
    "Print the values of your_first_tensor and tensor_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build first PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1530, 0.3138, 0.8175],\n",
      "        [0.7288, 0.7483, 0.7158],\n",
      "        [0.8853, 0.1551, 0.4494]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Import torch\n",
    "import torch\n",
    "\n",
    "# Create random tensor of size 3 by 3\n",
    "your_first_tensor = torch.rand(3, 3)\n",
    "\n",
    "# Calculate the shape of the tensor\n",
    "tensor_size = your_first_tensor.shape\n",
    "\n",
    "# Print the values of the tensor and its shape\n",
    "print(your_first_tensor)\n",
    "print(tensor_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matrix multiplication\n",
    "There are many important types of matrices which have their uses in neural networks. Some important matrices are matrices of ones (where each entry is set to 1) and the identity matrix (where the diagonal is set to 1 while all other values are 0). The identity matrix is very important in linear algebra: any matrix multiplied with identity matrix is simply the original matrix.\n",
    "\n",
    "Let us experiment with these two types of matrices. You are going to build a matrix of ones with shape 3 by 3 called tensor_of_ones and an identity matrix of the same shape, called identity_tensor. We are going to see what happens when we multiply these two matrices, and what happens if we do an element-wise multiplication of them.\n",
    "\n",
    "Create a matrix of ones with shape 3 by 3, and put it on variable tensor_of_ones.\n",
    "Create an identity matrix with shape 3 by 3, and put it on variable identity_tensor.\n",
    "Do a matrix multiplication of tensor_of_ones with identity_tensor and print its value.\n",
    "Do an element-wise multiplication of tensor_of_ones with identity_tensor and print its value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix of ones with shape 3 by 3\n",
    "tensor_of_ones = torch.ones(3, 3)\n",
    "\n",
    "# Create an identity matrix with shape 3 by 3\n",
    "# identity matrix is where the diagonal is set to 1 while all other values are 0\n",
    "# The identity matrix is very important in linear algebra: any matrix multiplied\n",
    "# with identity matrix is simply the original matrix.\n",
    "identity_tensor = torch.eye(3)\n",
    "\n",
    "# Do a matrix multiplication of tensor_of_ones with identity_tensor\n",
    "matrices_multiplied = torch.matmul(tensor_of_ones, identity_tensor)\n",
    "print(matrices_multiplied)\n",
    "\n",
    "# Do an element-wise multiplication of tensor_of_ones with identity_tensor\n",
    "element_multiplication = tensor_of_ones * identity_tensor\n",
    "print(element_multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matrices_multiplied is same as tensor_of_ones (because identity matrix is the neutral element in matrix multiplication, the product of any matrix multiplied with it gives the original matrix), while element_multiplication is same as identity_tensor.\n",
    "\n",
    "#### Forward propagation\n",
    "https://campus.datacamp.com/courses/introduction-to-deep-learning-with-pytorch/introduction-to-pytorch?ex=4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.]) tensor([-4.]) tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.Tensor([2])\n",
    "b = torch.Tensor([-4])\n",
    "c = torch.Tensor([-2])\n",
    "d = torch.Tensor([2])\n",
    "\n",
    "e= a + b\n",
    "f= c * d\n",
    "\n",
    "g= e *f\n",
    "print(e,f,g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass\n",
    "Let's have something resembling more a neural network. The computational graph has been given below. You are going to initialize 3 large random tensors, and then do the operations as given in the computational graph. The final operation is the mean of the tensor, given by torch.mean(your_tensor).\n",
    "\n",
    "Initialize random tensors x, y and z, each having shape (1000, 1000).\n",
    "Multiply x with y, putting the result in tensor q.\n",
    "Do an elementwise multiplication of tensor z with tensor q, putting the results in f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(125.2134)\n"
     ]
    }
   ],
   "source": [
    "# Initialize tensors x, y and z\n",
    "x = torch.rand(1000, 1000)\n",
    "y = torch.rand(1000, 1000)\n",
    "z = torch.rand(1000, 1000)\n",
    "\n",
    "# Multiply x with y\n",
    "q = torch.matmul(x,y)\n",
    "# Multiply elementwise z with q\n",
    "f = z * q\n",
    "\n",
    "mean_f = torch.mean(f)\n",
    "print(mean_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just built a nice computational graph containing 5'000'001 values. In the next lesson, you are going to compute the gradients of this graph.\n",
    "\n",
    "### Backpropagation\n",
    "https://classroom.udacity.com/courses/ud188/lessons/b4ca7aaa-b346-43b1-ae7d-20d27b2eab65/concepts/4cc13714-37d7-4705-a714-314ede5290b5\n",
    "Now, we're ready to get our hands into training a neural network. For this, we'll use the method known as backpropagation. In a nutshell, backpropagation will consist of:\n",
    "\n",
    "Doing a feedforward operation.\n",
    "Comparing the output of the model with the desired output.\n",
    "Calculating the error.\n",
    "Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.\n",
    "Use this to update the weights, and get a better model.\n",
    "Continue this until we have a model that is good.\n",
    "Sounds more complicated than what it actually is. Let's take a look in the next few videos. The first video will show us a conceptual interpretation of what backpropagation is.\n",
    "\n",
    "#### Backpropagation  by auto-differentiation\n",
    "main algorithm in neural networks,\n",
    "Derivatives - represent a rate of change in a function\n",
    "can be described as steepnees of the function\n",
    "\n",
    "Derivatives rules\n",
    "(f+g)' = f' + g'\n",
    "(f*g)'= f*dg + g*df\n",
    "(x^n)' = d/dx  x^n = nx^(n-1)\n",
    "(1/x)' = - 1/x^2\n",
    "(f/g)' = (df * 1/g) + (-1/g^2  dg *f)\n",
    "\n",
    "\n",
    "chain rule\n",
    "\n",
    "d/dx [(f(x))^n] = n(f(x))^n-1 * f'(x)\n",
    "\n",
    "d/dx [f(g(x))] = f'(g(x))g'(x)\n",
    "\n",
    "gradient is multivariable generalization of the derivative\n",
    "\n",
    "#### backpropagation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(-3., requires_grad=True)\n",
    "y = torch.tensor(5., requires_grad=True)\n",
    "z = torch.tensor(-2., requires_grad=True)\n",
    "q = x + y \n",
    "f = q * z\n",
    "f.backward()\n",
    "\n",
    "print('Gradient of z is:' +str(z.grad))\n",
    "print('Gradient of y is:' +str(y.grad))\n",
    "print('Gradient of x is:' +str(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you know how to compute derivatives! While PyTorch computes derivatives for you, mastering them will make you a much better deep learning practitioner and that knowledge will guide you in training neural networks better.\n",
    "\n",
    "##### Backpropagation using PyTorch\n",
    "Here, you are going to use automatic differentiation of PyTorch in order to compute the derivatives of x, y and z from the previous exercise.\n",
    "\n",
    "Initialize tensors x, y and z to values 4, -3 and 5.\n",
    "Put the sum of tensors x and y in q, put the product of q and z in f.\n",
    "Calculate the derivatives of the computational graph.\n",
    "Print the gradients of the x, y and z tensors.\n",
    "\n",
    "Initialize tensors x, y and z to values 4, -3 and 5.\n",
    "Put the sum of tensors x and y in q, put the product of q and z in f.\n",
    "Calculate the derivatives of the computational graph.\n",
    "Print the gradients of the x, y and z tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of z is:tensor(1.)\n",
      "Gradient of y is:tensor(5.)\n",
      "Gradient of x is:tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(4., requires_grad=True)\n",
    "y = torch.tensor(-3., requires_grad=True)\n",
    "z = torch.tensor(5., requires_grad=True)\n",
    "q = x + y \n",
    "f = q * z\n",
    "\n",
    "#calculate derivative\n",
    "f.backward()\n",
    "\n",
    "print('Gradient of z is:' +str(z.grad))\n",
    "print('Gradient of y is:' +str(y.grad))\n",
    "print('Gradient of x is:' +str(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calculating gradients in PyTorch\n",
    "Remember the exercise in forward pass? Now that you know how to calculate derivatives, let's make a step forward and start calculating the gradients (derivatives of tensors) of the computational graph you built back then. We have already initialized for you three random tensors of shape (1000, 1000) called x, y and z. First, we multiply tensors x and y, then we do an elementwise multiplication of their product with tensor z, and then we compute its mean. In the end, we compute the derivatives.\n",
    "\n",
    "The main difference from the previous exercise is the scale of the tensors. While before, tensors x, y and z had just 1 number, now they each have 1 million numbers.\n",
    "\n",
    "Multiply tensors x and y, put the product in tensor q.\n",
    "Do an elementwise multiplication of tensors z with q.\n",
    "Calculate the gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1000, 1000, requires_grad=True)\n",
    "y = torch.rand(1000, 1000, requires_grad=True)\n",
    "z = torch.rand(1000, 1000, requires_grad=True)\n",
    "# Multiply tensors x and y\n",
    "q = torch.matmul(x,y)\n",
    "\n",
    "\n",
    "# Elementwise multiply tensors z with q\n",
    "f = z * q\n",
    "\n",
    "mean_f = torch.mean(f)\n",
    "\n",
    "# Calculate the gradients\n",
    "mean_f.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, calculating gradients is as easy as calculating derivatives in PyTorch. Obviously, if the tensors are very large (billions of values) then the calculation might take some time.\n",
    "\n",
    "#### Introduction to Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "Your first neural network\n",
    "You are going to build a neural network in PyTorch, using the hard way. Your input will be images of size (28, 28), so images containing 784 pixels. Your network will contain an input_layer, a hidden layer with 200 units, and an output layer with 10 classes. The input layer has already been created for you. You are going to create the weights, and then do matrix multiplications, getting the results from the network.\n",
    "\n",
    "Initialize with random numbers two matrices of weights, called weight_1 and weight_2.\n",
    "Set the result of input_layer times weight_1 to hidden_1. Set the result of hidden_1 times weight_2 to output_layer.\n",
    "\n",
    "https://campus.datacamp.com/courses/introduction-to-deep-learning-with-pytorch/introduction-to-pytorch?ex=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20813.0352, 20862.6406, 19335.0664, 18141.1523, 19175.9766, 18487.1172,\n",
      "        19634.6035, 19910.5176, 19620.8281, 19092.0410])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your input will be images of size (28, 28), so images containing 784 pixels\n",
    "input_layer = torch.rand(784)\n",
    "\n",
    "# Initialize the weights of the neural network\n",
    "weight_1 = torch.rand(784, 200)\n",
    "weight_2 = torch.rand(200, 10)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Multiply hidden_1 with weight_2\n",
    "output_layer = torch.matmul(hidden_1, weight_2)\n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, neural networks are just matrix (tensor) multiplication. This is the reason why we have put so much emphasis on matrices and tensors!\n",
    "\n",
    "Your first PyTorch neural network\n",
    "You are going to build the same neural network you built in the previous exercise, but now using the PyTorch way. As a reminder, you have 784 units in the input layer, 200 hidden units and 10 units for the output layer.\n",
    "\n",
    "Instantiate two linear layers calling them self.fc1 and self.fc2. Determine their correct dimensions.\n",
    "Implement the .forward() method, using the two layers you defined and returning x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Instantiate all 2 linear layers  \n",
    "        self.fc1 = nn.Linear(784,200)\n",
    "        self.fc2 = nn.Linear(200,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Use the instantiated layers and return x\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! You just built your first PyTorch artificial neural network. You are going to build many more during this course.\n",
    "\n",
    "#### 02 Artificial Neural Networks\n",
    "\n",
    "##### Activation functions\n",
    "\n",
    "\n",
    "###### simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9696, 0.9399])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "input_layer = torch.tensor([2.,1.])\n",
    "weight_1= torch.tensor([[0.45, 0.32], [-0.12, 0.29]])\n",
    "hidden_layer = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "weight_2= torch.tensor([[0.48, 0.12], [0.64, 0.91]])\n",
    "output_layer = torch.matmul(hidden_layer, weight_2)\n",
    "print(output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9696, 0.9399])\n",
      "tensor([[0.4208, 0.3452],\n",
      "        [0.1280, 0.2495]])\n"
     ]
    }
   ],
   "source": [
    "#matrix multiplication is a linear transformation\n",
    "\n",
    "weight_1= torch.tensor([[0.45, 0.32], [-0.12, 0.29]])\n",
    "weight_2= torch.tensor([[0.48, 0.12], [0.64, 0.91]])\n",
    "\n",
    "weight = torch.matmul(weight_1, weight_2)\n",
    "\n",
    "\n",
    "output_layer = torch.matmul(input_layer, weight)\n",
    "print(output_layer)\n",
    "print(weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non linear split of data\n",
    "We use activatin fuctions to split non-linear data \n",
    "Examples of activation functions: Sigmoid, tanh, ReLU, Leaky ReLU, Maxout, ELU\n",
    "\n",
    "The most used is ReLU (rectified linear unit), even for most complex imaging problems\n",
    "ReLU(x) = max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 0.])\n",
      "tensor([[2.0000, 0.0000],\n",
      "        [1.2000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "relu = nn.ReLU()\n",
    "tensor_1 = torch.tensor([2., -4.])\n",
    "print(relu(tensor_1))\n",
    "\n",
    "tensor_2 = torch.tensor([[2.,-4.],\n",
    "                        [1.2, 0.]])\n",
    "print(relu(tensor_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ReLU positive inputs conserved, negative zeroed\n",
    "Lets use other activation functions\n",
    "\n",
    "\n",
    "Neural networks\n",
    "Let us see the differences between neural networks which apply ReLU and those which do not apply ReLU. We have already initialized the input called input_layer, and three sets of weights, called weight_1, weight_2 and weight_3.\n",
    "\n",
    "We are going to convince ourselves that networks with multiple layers which do not contain non-linearity can be expressed as neural networks with one layer.\n",
    "\n",
    "The network and the shape of layers and weights is shown below.\n",
    "\n",
    "\n",
    "Calculate the first and second hidden layer by multiplying the appropriate inputs with the corresponding weights.\n",
    "Calculate and print the results of the output.\n",
    "Set weight_composed_1 to the product of weight_1 with weight_2, then set weight to the product of weight_composed_1 with weight_3.\n",
    "Calculate and print the output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = torch.tensor([[ 0.0401, -0.9005,  0.0397, -0.0876]])\n",
    "\n",
    "weight_1 = torch.tensor([[-0.1094, -0.8285,  0.0416, -1.1222],\n",
    "                        [ 0.3327, -0.0461,  1.4473, -0.8070],\n",
    "                        [ 0.0681, -0.7058, -1.8017,  0.5857],\n",
    "                        [ 0.8764,  0.9618, -0.4505,  0.2888]])\n",
    "\n",
    "weight_2 = torch.tensor([[ 0.6856, -1.7650,  1.6375, -1.5759],\n",
    "                        [-0.1092, -0.1620,  0.1951, -0.1169],\n",
    "                        [-0.5120,  1.1997,  0.8483, -0.2476],\n",
    "                        [-0.3369,  0.5617, -0.6658,  0.2221]])\n",
    "\n",
    "weight_3 = torch.tensor([[ 0.8824,  0.1268,  1.1951,  1.3061],\n",
    "                        [-0.8753, -0.3277, -0.1454, -0.0167],\n",
    "                        [ 0.3582,  0.3254, -1.8509, -1.4205],\n",
    "                        [ 0.3786,  0.5999, -0.5665, -0.3975]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2653, 0.1311, 3.8219, 3.0032]])\n",
      "tensor([[0.2653, 0.1311, 3.8219, 3.0032]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the first and second hidden layer\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "hidden_2 = torch.matmul(hidden_1, weight_2)\n",
    "\n",
    "# Calculate the output\n",
    "print(torch.matmul(hidden_2, weight_3))\n",
    "\n",
    "# Calculate weight_composed_1 and weight\n",
    "weight_composed_1 = torch.matmul(weight_1, weight_2)\n",
    "weight = torch.matmul(weight_composed_1, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are identical!\n",
    "\n",
    "ReLU activation\n",
    "In this exercise, we have the same settings as the previous exercise. In addition, we have instantiated the ReLU activation function called relu().\n",
    "\n",
    "Now we are going to build a neural network which has non-linearity and by doing so, we are going to convince ourselves that networks with multiple layers and non-linearity functions cannot be expressed as a neural network with one layer.\n",
    "\n",
    "Apply non-linearity on hidden_1 and hidden_2.\n",
    "Apply non-linearity in the product of first two weight.\n",
    "Multiply the result of the previous step with weight_3.\n",
    "Multiply input_layer with weight and print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2770, -0.0345, -0.1410, -0.0664]])\n",
      "tensor([[-0.2117, -0.4782,  4.0438,  3.0417]])\n"
     ]
    }
   ],
   "source": [
    "#activation function relu\n",
    "relu = nn.ReLU()\n",
    "# Apply non-linearity on hidden_1 and hidden_2\n",
    "hidden_1_activated = relu(torch.matmul(input_layer, weight_1))\n",
    "hidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))\n",
    "print(torch.matmul(hidden_2_activated, weight_3))\n",
    "\n",
    "# Apply non-linearity in the product of first two weights. \n",
    "weight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))\n",
    "\n",
    "# Multiply `weight_composed_1_activated` with `weight_3\n",
    "weight = torch.matmul(weight_composed_1_activated, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! As expected the results are different from the previous exercise.\n",
    "\n",
    "\n",
    "ReLU activation again\n",
    "Neural networks don't need to have the same number of units in each layer. Here, you are going to experiment with the ReLU activation function again, but this time we are going to have a different number of units in the layers of the neural network. The input layer will still have 4 features, but then the first hidden layer will have 6 units and the output layer will have 2 units.\n",
    "\n",
    "\n",
    "Instantiate the ReLU() activation function as relu (the function is part of nn module).\n",
    "Initialize weight_1 and weight_2 with random numbers.\n",
    "Multiply the input_layer with weight_1, storing results in hidden_1.\n",
    "Apply the relu activation function over hidden_1, and then multiply the output of it with weight_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate ReLU activation function as relu\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Initialize weight_1 and weight_2 with random numbers\n",
    "\n",
    "#If the input layer has 4 units, and the hidden layer has 6 units, then \n",
    "#the weight_1 should have shape (4, 6).\n",
    "weight_1 = torch.rand(4, 6)\n",
    "#hidden layer 6 units, output layer 2 units\n",
    "weight_2 = torch.rand(6, 2)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Apply ReLU activation function over hidden_1 and multiply with weight_2\n",
    "hidden_1_activated = relu(hidden_1)\n",
    "print(torch.matmul(hidden_1_activated, weight_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You can now build neural networks with different number of neurons on each layer.\n",
    "#####  LOSS Functions\n",
    "\n",
    "How to train a neural network:\n",
    "Process:\n",
    "\n",
    "Initialize neural networks with random weights\n",
    "Do a forward pass\n",
    "Calculate loss function (1 number)\n",
    "Calcualte the gradients\n",
    "Change the weights based on gradients\n",
    "\n",
    "LOSS Functions:\n",
    "For regression (least squared loss)\n",
    "For classification: softmax cross-entropy loss\n",
    "For more complicated problems (like obbject detection), more complicated losses\n",
    "\n",
    "###### softmax cross-entropy loss\n",
    "Loss functions must be differentiatable, otherwise we cant compute gradients\n",
    "\n",
    "softmax is a function that transforms numbers into probabilities\n",
    "First count softmax function as a probability\n",
    "Than cross entropy loss which is L = -ln(probability)\n",
    "If a probability is close to 1, loss is close to 0\n",
    "if probability is small loss is close to infinity\n",
    "\n",
    "##### CE loss in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0404)\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[3.2, 5.1, -1.7]])\n",
    "#first class\n",
    "ground_truth = torch.tensor([0])\n",
    "#loss function combines softmax with cross entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0061)\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[10.2, 5.1, -1.7]])\n",
    "#first class\n",
    "ground_truth = torch.tensor([0])\n",
    "#loss function combines softmax with cross entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOSS is very small in last example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.1011)\n"
     ]
    }
   ],
   "source": [
    "# if -10\n",
    "logits = torch.tensor([[-10, 5.1, -1.7]])\n",
    "#first class\n",
    "ground_truth = torch.tensor([0])\n",
    "#loss function combines softmax with cross entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating LOSS by hand\n",
    "\n",
    "Let's start the exercises by calculating the loss function by hand. Don't do this exercise in PyTorch, it is important to first do it using only pen and paper (and a calculator).\n",
    "\n",
    "We have the same example as before but now our object is actually a frog, and the predicted scores are -1.2 for class 0 (cat), 0.12 for class 1 (car) and 4.8 for class 2 (frog).\n",
    "\n",
    "What is the result of the softmax cross-entropy loss function?\n",
    "\n",
    "Class\tPredicted Score\n",
    "Cat\t-1.2\n",
    "Car\t0.12\n",
    "Frog\t4.8\n",
    "\n",
    "\n",
    "First exponentiate each term, then normalize the probabilities by dividing with sum, and then do -ln(probability_frog).\n",
    "The base of logarithm should be e (e is approximately 2.7182818).\n",
    "\n",
    "Calculating loss function in PyTorch\n",
    "You are going to code the previous exercise, and make sure that we computed the loss correctly. Predicted scores are -1.2 for class 0 (cat), 0.12 for class 1 (car) and 4.8 for class 2 (frog). The ground truth is class 2 (frog). Compute the loss function in PyTorch.\n",
    "\n",
    "Initialize the tensor of scores with numbers [[-1.2, 0.12, 4.8]], and the tensor of ground truth [2].\n",
    "Instantiate the cross-entropy loss and call it criterion.\n",
    "Compute and print the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0117)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the scores and ground truth\n",
    "logits = torch.tensor([[-1.2, 0.12, 4.8]])\n",
    "ground_truth = torch.tensor([2])\n",
    "\n",
    "# Instantiate cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute and print the loss\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the loss function PyTorch calculated gives the same number as the loss function you calculated. Being proficient in understanding and calculating loss functions is a very important skill in deep learning.\n",
    "\n",
    "Loss function of random scores\n",
    "If the neural network predicts random scores, what would be its loss function? Let's find it out in PyTorch. The neural network is going to have 1000 classes, each having a random score. For ground truth, it will have class 111. Calculate the loss function.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import torch and torch.nn as nn\n",
    "Initialize logits with a random tensor of shape (1, 1000) and ground_truth with a tensor containing the number 111.\n",
    "Instantiate the cross-entropy loss in a variable called criterion.\n",
    "Calculate and print the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.0683)\n"
     ]
    }
   ],
   "source": [
    "# Import torch and torch.nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Initialize logits and ground truth\n",
    "logits = torch.rand(1,1000)\n",
    "ground_truth = torch.tensor([111])\n",
    "\n",
    "\n",
    "# Instantiate cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate and print the loss\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is close to -ln(1/1000) = 6.9. This is not surprising, considering that scores were random and close to each other, so the probability for each class was approximately the same (1/1000) = 0.001.\n",
    "\n",
    "\n",
    "#### Preparting a dataset in PyTorch\n",
    "\n",
    "https://campus.datacamp.com/courses/introduction-to-deep-learning-with-pytorch/artificial-neural-networks?ex=9\n",
    "\n",
    "we will use MNIST (70 thousand hand written digits, shaped 28X28, back and white, 1 channel)\n",
    "CIFAR-10( dataset with natural colored images, 32X32, RGB format 3 channels red green and blue)\n",
    "\n",
    "\n",
    "In addition to torch we import torchvision\n",
    "dasets needs to be in pytorch friendly format\n",
    "see how we can achive this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "[transforms.ToTensor(),\n",
    "transforms.Normalize((0.4914, 0.48216, 0.44653),\n",
    "                    (0.24703, 0.24349, 0.26159))])\n",
    "\n",
    "\n",
    "#next is to get a dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# build trainloader and testloader to make data ready for pytorch\n",
    "\n",
    "#use only 32 randomly sampled images, dataset is too big\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                         shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "#how to inspect dataset\n",
    "print(testloader.dataset.test_data.shape, trainloader.dataset.train_data.shape)\n",
    "\n",
    "#similarly we can look at batch size\n",
    "print(testloader.batch_size)\n",
    "\n",
    "# will be good to look at a type of random sampler\n",
    "print(trainloader.sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preparing MNIST dataset\n",
    "You are going to prepare dataloaders for MNIST training and testing set. As we explained in the lecture, MNIST has some differences to CIFAR-10, with the main difference being that MNIST images are grayscale (1 channel based) instead of RGB (3 channels).\n",
    "\n",
    "####### Instructions\n",
    "100 XP\n",
    "Transform the data to torch tensors and normalize it, mean is 0.1307 while std is 0.3081.\n",
    "Prepare the trainset and the testset.\n",
    "Prepare the dataloaders for training and testing so that only 32 pictures are processed at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Transform the data to torch tensors and normalize it\n",
    "# mean is 0.1307 while std is 0.3081.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307), (0.3081))\n",
    "])\n",
    "\n",
    "# Preparing training set and test set\n",
    "trainset = torchvision.datasets.MNIST('mnist', train=True, download=True,\n",
    "                                      transform=transform)\n",
    "testset = torchvision.datasets.MNIST('mnist', train=False, download=True,\n",
    "                                     transform=transform)\n",
    "\n",
    "# Prepare training loader and test loader\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                           shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=32, \n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now you know how to prepare datasets in PyTorch. You are very close to reaching the grand goal, training neural networks!\n",
    "\n",
    "Inspecting the dataloaders\n",
    "Now you are going to explore a bit the dataloaders you created in the previous exercise. In particular, you will compute the shape of the dataset in addition to the minibatch size.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Compute the shapes of the trainset and testset.\n",
    "Print the computed values.\n",
    "Compute the size of the minibatch for both trainset and testset.\n",
    "Print the minibatch size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n",
      "32 32\n"
     ]
    }
   ],
   "source": [
    "# Compute the shape of the training set and test set\n",
    "trainset_shape = train_loader.dataset.data.shape\n",
    "testset_shape = test_loader.dataset.data.shape\n",
    "\n",
    "# Print the computed shapes\n",
    "print(trainset_shape, testset_shape)\n",
    "\n",
    "# Compute the size of the minibatch for training set and test set\n",
    "trainset_batchsize = train_loader.batch_size\n",
    "testset_batchsize = test_loader.batch_size\n",
    "\n",
    "# Print sizes of the minibatch\n",
    "print(trainset_batchsize, testset_batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have armed yourself with the knowledge of building and exploring dataloaders, it is time to start training neural networks!\n",
    "\n",
    "##### Training neural networks\n",
    "\n",
    "Prepare data loaders\n",
    "Build a neural network\n",
    "\n",
    "loop over:\n",
    "1. do a forward pass\n",
    "2. calculate loss function\n",
    "3. calculate gradients\n",
    "4. change the weights based on gradients\n",
    "\n",
    "Iterate over this 4 steps many times\n",
    "\n",
    "Gradient descent - a steepness of a function\n",
    "find a local minima of a function, all algorithm is based on minimizing loss of a function\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  PREPARE DATASET ##################\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "[transforms.ToTensor(),\n",
    "transforms.Normalize((0.4914, 0.48216, 0.44653),\n",
    "                    (0.24703, 0.24349, 0.26159))])\n",
    "\n",
    "\n",
    "#next is to get a dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "\n",
    "###### prepare a class for neural network #################\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1=nn.Linear(32*32*3, 500)\n",
    "        self.fc2=nn.Linear(500,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "#start the training\n",
    "net=Net()\n",
    "#cross entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer Adam, works well, version of gradient descent\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
    "\n",
    "for epoch in range(10):  #loop over the dataset multiple times\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        #get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(1, 32*32*3)\n",
    "        \n",
    "        #zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward and backward + optimize\n",
    "        #forward to output\n",
    "        outputs = net(inputs)\n",
    "        # compute loss function\n",
    "        loss=criterion(outputs, labels)\n",
    "        # gradients using loss.divert\n",
    "        loss.backward()\n",
    "        # change weights\n",
    "        optimizer.step()\n",
    "        \n",
    "# using the trained net to get predictions\n",
    "correct, total = 0,0\n",
    "predictions = []\n",
    "#set a model in evaluation mode\n",
    "net.eval()\n",
    "\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    inputs, labels.view(-1, 32*32*3)\n",
    "    outputs = net(inputs)\n",
    "    -, predicted = torch.max(outputs.data, 1)\n",
    "    predictions.append(outputs)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    \n",
    "print('the testing set accuracy of \\\n",
    "      the network is : %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a neural network - again\n",
    "You haven't created a neural network since the end of the first chapter, so this is a good time to build one (practice makes perfect). Build a class for a neural network which will be used to train on the MNIST dataset. The dataset contains images of shape (28, 28, 1), so you should deduct the size of the input layer. For hidden layer use 200 units, while for output layer use 10 units (1 for each class). For activation function, use relu in a functional way (nn.Functional is already imported as F).\n",
    "\n",
    "For context, the same net will be trained and used to make predictions in the next two exercises.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Define the class called Net which inherits from nn.Module.\n",
    "In the __init__() method, define the parameters for the two fully connected layers.\n",
    "In the .forward() method, do the forward step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the class Net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):    \n",
    "    \t# Define all the parameters of the net\n",
    "        super(Net, self).__init__()\n",
    "        #The dataset contains images of shape (28, 28, 1), so you\n",
    "        #should deduct the size of the input layer. For hidden layer\n",
    "        #use 200 units\n",
    "        self.fc1 = nn.Linear(28 * 28 * 1, 200)\n",
    "        \n",
    "        # while for output layer use 10 units (1 for each class). \n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):   \n",
    "    \t# Do the forward pass\n",
    "        #for activation function, use relu in a functional way\n",
    "        #(nn.Functional\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network\n",
    "Given the fully connected neural network (called model) which you built in the previous exercise and a train loader called train_loader containing the MNIST dataset (which we created for you), you're to train the net in order to predict the classes of digits. You will use the Adam optimizer to optimize the network, and considering that this is a classification problem you are going to use cross entropy as loss function.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instantiate the Adam optimizer with learning rate 3e-4 and instantiate Cross-Entropy as loss function.\n",
    "Complete a forward pass on the neural network using the input data.\n",
    "Using backpropagation, compute the gradients of the weights, and then change the weights using the Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Instantiate the Adam optimizer and Cross-Entropy loss function\n",
    "model = Net()   \n",
    "#initiate Adam optimizer\n",
    "#learning rate 3e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "#instantiate cross-entropy as loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "for batch_idx, data_target in enumerate(train_loader):\n",
    "    data = data_target[0]\n",
    "    target = data_target[1]\n",
    "    \n",
    "    data = data.view(-1, 28 * 28)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Complete a forward pass on the neural network\n",
    "    #using the input data\n",
    "    output = model(data)\n",
    "\n",
    "    # Compute the loss, gradients and change the weights\n",
    "   # Using backpropagation, compute the gradients of the #weights, and then change the weights using the Adam #optimizer.\n",
    "    loss = criterion(output,target )\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the network to make predictions\n",
    "Now that you have trained the network, use it to make predictions for the data in the testing set. The network is called model (same as in the previous exercise), and the loader is called test_loader. We have already initialized variables total and correct to 0.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Set the network in testing (eval) mode.\n",
    "Put each image into a vector using inputs.view(-1, number_of_features) where the number of features should be deducted by multiplying spatial dimensions (shape) of the image.\n",
    "Do the forward pass and put the predictions in output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing set accuracy of the network is: 94 %\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "\n",
    "# Set the model in eval mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # Put each image into a vector\n",
    "    # where the number of features should be deducted by #multiplying spatial dimensions (shape) of the image\n",
    "    inputs = inputs.view(-1, 28 * 28)\n",
    "    \n",
    "    # Do the forward pass and get the predictions\n",
    "    outputs = model(inputs)\n",
    "    _, outputs = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (outputs == labels).sum().item()\n",
    "print('The testing set accuracy of the network is: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you are able to use neural networks to make predictions. NB: The accuracy of the net is too low compared to what we can do with neural networks. We used only a subpart of the dataset in order for the training to happen fast. You can achieve a much higher accuracy by using the entire dataset, and by using a larger neural network.\n",
    "\n",
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "\n",
    "#### Convolutional operator\n",
    "Fully connected neural networks come with a few problems:\n",
    "do you need to consider all the relations between the features?\n",
    "fully connected neural networks are big and so very computationally inefficient\n",
    "they have so many parameters and so overfit\n",
    "\n",
    "2 Main ideas mitigating these problems\n",
    "Units are connected with only a few units from the previos layer\n",
    "Units share weights\n",
    "\n",
    "CNNs solves this problem\n",
    "\n",
    "Idea of convolution is simple: \n",
    "convolve an image with a filter (weights of network)\n",
    "Convolving = dot product between image and filter\n",
    "\n",
    "After that we get an activation map (also feature map)\n",
    "(size of filter must be smaller that size of the image)\n",
    "\n",
    "Convolutional layer contains multiple activation maps\n",
    "\n",
    "If we want the size of activation map to match the size of an image we can do padding, adding zeroes at the edge of the image\n",
    "\n",
    "There are 2 ways of using CNN's  in PyTorch\n",
    "OOP-based (torch.nn) and Functional (torch.nn.functional)\n",
    "\n",
    "##### OOP-based (torch.nn)\n",
    "in_channels (int) Number of channels in input\n",
    "out_channels (int) number of channels produced by the convolution\n",
    "kernel_size (int or tuple) size of convolving kernel\n",
    "\n",
    "stride (int or tuple, optional)stride of the convolution default 1\n",
    "padding (int or tuple, optional) zero-padding\n",
    "\n",
    "##### Functional (torch.nn.functional)\n",
    "input - input tensor of shape (minibatch X in_channels X iH X iW)\n",
    "weight - filter of shape (out_channels * in_channels * kH 8 kW)\n",
    "stride - the stride of the convolving kernel. Can be a single number or a tuple (sH, sW) default:1\n",
    "\n",
    "padding - implicit zero padding on both sides of the input\n",
    "Can be a single number or a tuple (padH, padW) default: 0\n",
    "\n",
    "stride = distance where convolutional filter is applied\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### OOP-based (torch.nn)\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "image = torch.rand(16, 3, 32, 32)\n",
    "conv_filter = torch.nn.Conv2d(in_channel = 3,\n",
    "                             out_channel=1,\n",
    "                             kernel_size=5\n",
    "                             stride=1, padding=0)\n",
    "output_feature = conv_filter(image)\n",
    "print(output_feature.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functional (torch.nn.functional)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "image = torch.rand(16, 3, 32, 32)\n",
    "filter = torch.rand(1,3,5,5)\n",
    "out_feat_F = F.conv2d(image, filter, stride=1, padding=0)\n",
    "print(out_feat_F.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution operator - OOP way - practice\n",
    "Let's kick off this chapter by using convolution operator from the torch.nn package. You are going to create a random tensor which will represent your image and random filters to convolve the image with. Then you'll apply those images.\n",
    "\n",
    "The torch library and the torch.nn package have already been imported for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Create 10 images with shape (1, 28, 28).\n",
    "Build 6 convolutional filters of size (3, 3) with stride set to 1 and padding set to 1.\n",
    "Apply the filters in the image and print the shape of the feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "# Create 10 random images of shape (1, 28, 28)\n",
    "images = torch.rand(10, 1, 28, 28)\n",
    "\n",
    "# Build 6 conv. filters\n",
    "#Build 6 convolutional filters of size (3, 3) \n",
    "#with stride set to 1 and padding set to 1\n",
    "conv_filters = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(3, 3), stride=1, padding=1)\n",
    "\n",
    "# Convolve the image with the filters \n",
    "output_feature = conv_filters(images)\n",
    "print(output_feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolution operator - Functional way\n",
    "While I and most of PyTorch practitioners love the torch.nn package (OOP way), other practitioners prefer building neural network models in a more functional way, using torch.nn.functional. More importantly, it is possible to mix the concepts and use both libraries at the same time (we have already done it in the previous chapter). You are going to build the same neural network you built in the previous exercise, but this time using the functional way.\n",
    "\n",
    "As before, we have already imported the torch library and torch.nn.functional as F.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Create 10 random images with shape (1, 28, 28).\n",
    "Create 6 random filters with shape (1, 3, 3).\n",
    "Convolve the images with the filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create 10 random images of shape 1,28,28\n",
    "image = torch.rand(10, 1, 28, 28)\n",
    "\n",
    "# Create 6 filters of shape 1,3,3\n",
    "filters = torch.rand(6, 1, 3, 3)\n",
    "\n",
    "# Convolve the image with the filters\n",
    "output_feature = F.conv2d(image, filters, stride=1, padding=1)\n",
    "print(output_feature.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling operators\n",
    "Another important layers in CNS apart from convolutional operator\n",
    "Convolution extracts features from image, while pooling is feature selection, choosing the most dominant features from the image by combining different features, eventualy lowering number of features and reducing computation time.\n",
    "\n",
    "Pooling is simply lowering spatial dimensions tipycally by 2. so computations faster\n",
    "2 most importan pooling operators are max pooling and average pooling.\n",
    "\n",
    "Max selects maximum number of 4, and average calculates average of 4.\n",
    "\n",
    "#### Max-pooling in PyTorch (OOP and Functional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OOP Max-pooling in Pytorch\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "im=torch.Tensor([[[3,1,3,5], [6,0,7,9], [3,2,1,4], [0,2,4,3]]])\n",
    "max_pooling = torch.nn.MaxPool2d(2)\n",
    "output_feature = max_pooling(im)\n",
    "print(output_feature)\n",
    "\n",
    "### Functional Max-pooling in PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "im=torch.Tensor([[[3,1,3,5], [6,0,7,9], [3,2,1,4], [0,2,4,3]]])\n",
    "output_feature_F=F.max_pool2d(im,2)\n",
    "print(output_feature_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OOP Average-pooling in Pytorch\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "im=torch.Tensor([[[3,1,3,5], [6,0,7,9], [3,2,1,4], [0,2,4,3]]])\n",
    "avg_pooling = torch.nn.AvgPool2d(2)\n",
    "output_feature = avg_pooling(im)\n",
    "print(output_feature)\n",
    "\n",
    "### Functional Average-pooling in Pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "im=torch.Tensor([[[3,1,3,5], [6,0,7,9], [3,2,1,4], [0,2,4,3]]])\n",
    "output_feature_F=F.avg_pool2d(im,2)\n",
    "print(output_feature_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max-pooling operator\n",
    "Here you are going to practice using max-pooling in both OOP and functional way, and see for yourself that the produced results are the same. We have already created and printed the image for you, and imported torch library in addition to torch.nn and torch.nn.Functional as F packages.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Build a max-pooling operator with size 2.\n",
    "Apply the max-pooling operator in the image (loaded as im).\n",
    "Use a max-pooling operator in functional way in the image.\n",
    "Print the results of both cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.9583, 0.8516, 0.9719],\n",
      "          [0.6846, 0.7025, 0.8354],\n",
      "          [0.8008, 0.8600, 0.6248]]]])\n",
      "tensor([[[[0.9583, 0.8516, 0.9719],\n",
      "          [0.6846, 0.7025, 0.8354],\n",
      "          [0.8008, 0.8600, 0.6248]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "im = torch.rand(1, 1, 6, 6)\n",
    "\n",
    "# Build a pooling operator with size `2`.\n",
    "max_pooling = torch.nn.MaxPool2d(2)\n",
    "\n",
    "# Apply the pooling operator\n",
    "output_feature = max_pooling(im)\n",
    "\n",
    "# Use pooling operator in the image\n",
    "output_feature_F=F.max_pool2d(im,2)\n",
    "\n",
    "# print the results of both cases\n",
    "print(output_feature)\n",
    "print(output_feature_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average-pooling operator\n",
    "After coding the max-pooling operator, you are now going to code the average-pooling operator. You just need to replace max-pooling with average pooling.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Build an average-pooling operator with size 2.\n",
    "Apply the average-pooling operator in the image.\n",
    "Use an average-pooling operator in functional way in the image, called im.\n",
    "Print the results of both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6729, 0.4745, 0.5138],\n",
      "          [0.3597, 0.3483, 0.4595],\n",
      "          [0.5460, 0.4669, 0.4094]]]])\n",
      "tensor([[[[0.6729, 0.4745, 0.5138],\n",
      "          [0.3597, 0.3483, 0.4595],\n",
      "          [0.5460, 0.4669, 0.4094]]]])\n"
     ]
    }
   ],
   "source": [
    "# Build a pooling operator with size `2`.\n",
    "avg_pooling = torch.nn.AvgPool2d(2)\n",
    "\n",
    "# Apply the pooling operator\n",
    "output_feature = avg_pooling(im)\n",
    "\n",
    "# Use pooling operator in the image\n",
    "output_feature_F = F.avg_pool2d(im,2)\n",
    "\n",
    "# print the results of both cases\n",
    "print(output_feature)\n",
    "print(output_feature_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic job, now you know to apply different types of pooling in addition to convolutions. The moment of truth has arrived, in the lext lecture we are going to learn building convolutional neural networks, the most important technique in the entire field of machine learning.\n",
    "\n",
    "#### Building Convolutional Neural Networks\n",
    "\n",
    "##### AlexNet in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n",
    "        self.fc1 = nn.Linear(256*6*6, 4096)\n",
    "        self.fc1 = nn.Linear(4096, 4096)        \n",
    "        self.fc1 = nn.Linear(4096, num_classes)\n",
    "    \n",
    "    \n",
    "#than forward method\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x=self.maxpool(x)\n",
    "        x= self.relu(self.conv2(x))\n",
    "        x=self.maxpool(x)\n",
    "        x=self.relu(self.conv3(x))\n",
    "        x=self.relu(self.conv4(x))\n",
    "        x=self.relu(self.conv5(x))\n",
    "        x=self.maxpool(x)\n",
    "        x=self.avgpool(x)\n",
    "        x=x.view(x.size(0), 256*6*6)\n",
    "        x=self.relu(self.fc1(x))\n",
    "        x=self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "net=AlexNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Your first CNN - __init__ method\n",
    "You are going to build your first convolutional neural network. You're going to use the MNIST dataset as the dataset, which is made of handwritten digits from 0 to 9. The convolutional neural network is going to have 2 convolutional layers, each followed by a ReLU nonlinearity, and a fully connected layer. We have already imported torch and torch.nn as nn. Remember that each pooling layer halves both the height and the width of the image, so by using 2 pooling layers, the height and width are 1/4 of the original sizes. MNIST images have shape (1, 28, 28)\n",
    "\n",
    "For the moment, you are going to implement the __init__ method of the net. In the next exercise, you will implement the .forward() method.\n",
    "\n",
    "NB: We need 2 pooling layers, but we only need to instantiate a pooling layer once, because each pooling layer will have the same configuration. Instead, we will use self.pool twice in the next exercise.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instructions\n",
    "100 XP\n",
    "Instantiate two convolutional filters: the first one should have 5 channels, while the second one should have 10 channels. The kernel_size for both of them should be 3, and both should use padding=1. Use the names of the arguments (instead of using 1, use padding=1).\n",
    "Instantiate a ReLU() nonlinearity.\n",
    "Instantiate a max pooling layer which halves the size of the image in both directions.\n",
    "Instantiate a fully connected layer which connects the units with the number of classes (we are using MNIST, so there are 10 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "\n",
    "#MNIST dataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        #Instantiate two convolutional filters: the first one should have 5 channels,\n",
    "        #while the second one should have 10 channels.\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Instantiate a max pooling layer which halves the size of the image in both directions.\n",
    "        # so we set kernel_size to 2 and stride to 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Instantiate a fully connected layer\n",
    "        # Instantiate a fully connected layer which #connects the units with the number\n",
    "        #of classes (we are #using MNIST, so there are 10 classes).\n",
    "        self.fc = nn.Linear(7*7*10, 10)\n",
    "\n",
    "        \n",
    "       # Great job! Now all that remains is implementing the forward() method and you have your first CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your first CNN - forward() method\n",
    "Now that you have declared all the parameters of your CNN, all you need to do is to implement the net's forward() method, and voila, you have your very first PyTorch CNN.\n",
    "\n",
    "Note: for evaluation purposes, the entire code of the class needs to be in the script. We are using the __init__ method as you have coded it on the previous exercise, while you are going to code the .forward() method here.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instructions\n",
    "100 XP\n",
    "Apply the first convolutional layer, followed by the relu nonlinearity, then in the next line apply max-pooling layer.\n",
    "Apply the second convolutional layer, followed by the relu nonlinearity, then in the next line apply max-pooling layer.\n",
    "Transform the feature map from 4 dimensional to 2 dimensional space. The first dimension contains the batch size (-1), deduct the second dimension, by multiplying the values for height, width and depth.\n",
    "Apply the fully-connected layer and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\t\t\n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Instantiate a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Instantiate a fully connected layer\n",
    "        self.fc = nn.Linear(7 * 7 * 10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Prepare the image for the fully connected layer\n",
    "        #Your image started with shape (28, 28) and on it were performed 2 pooling layers,\n",
    "        #each halving the size. Second conv layer had 10 channels. What should be the second\n",
    "        #argument of view?\n",
    "        x = x.view(-1, 7 * 7 * 10)\n",
    "\n",
    "        # Apply the fully connected layer and return the result\n",
    "        return self.self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Convolutional Neural Networks\n",
    "not much difference training neural networks from convolutional neueral networks\n",
    "so, will just start from pytorch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-abae9baa2741>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m#instantiate class net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;31m#set loss function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'num_classes'"
     ]
    }
   ],
   "source": [
    "# do all necessary imports\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# create trainloader and testloader\n",
    "transform = transforms.Compose(\n",
    "[transforms.ToTensor(),\n",
    "transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root = './data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root = './data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "#### building a CNN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\t\t\n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        #progressivly increase number of channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Instantiate a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Instantiate a fully connected layer\n",
    "        #first number number of units from last layer\n",
    "        #set number of classes to 10\n",
    "        num_classes=10\n",
    "        self.fc = nn.Linear(128 * 4 * 4, num_classes)\n",
    "        \n",
    "        ### apply forward method\n",
    "        def forward(self, x):\n",
    "            #relu convolutions\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = self.pool(F.relu(self.conv3(x)))\n",
    "            #pooling layer\n",
    "            x = x.view(-1, 128 * 4 * 4)\n",
    "            # Apply the fully connected layer and return the result\n",
    "            return self.self.fc(x)        \n",
    "    \n",
    "#instantiate class net\n",
    "net = Net()\n",
    "#set loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#set optimizer Adam, learning rate #set optimizer Adam, learning rate 3e-4\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
    "\n",
    "#loop multiple times\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        #get the inputs\n",
    "        inputs, labels = data\n",
    "        #zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        #forward + backward +optimize\n",
    "        outputs=net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('finished training')\n",
    "\n",
    "\n",
    "\n",
    "#evaluating results\n",
    "corrent, total = 0,0\n",
    "predictions = []\n",
    "net.eval()\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    inputs,labels = data\n",
    "    outputs = net(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predictions.append(outputs)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    \n",
    "print('the testing set accuracy of the network is: %d %%' % (\n",
    "        100 * correct /total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training CNNs\n",
    "Similarly to what you did in Chapter 2, you are going to train a neural network. This time however, you will train the CNN you built in the previous lesson, instead of a fully connected network. The packages you need have been imported for you and the network (called net) instantiated. The cross-entropy loss function (called criterion) and the Adam optimizer (called optimizer) are also available. We have subsampled the training set so that the training goes faster, and you are going to use a single epoch.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Compute the predictions from the net.\n",
    "Using the predictions and the labels, compute the loss function.\n",
    "Compute the gradients for each weight.\n",
    "Update the weights using the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Instantiate a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Instantiate a fully connected layer\n",
    "        self.fc = nn.Linear(49 * 10, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply conv followed by relu, then in next line pool\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Apply conv followed by relu, then in next line pool\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Prepare the image for the fully connected layer\n",
    "        x = x.view(-1, 7 * 7 * 10)\n",
    "        \n",
    "        # Apply the fully connected layer and return the result\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Transform the data to torch tensors and normalize it\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307), (0.3081))\n",
    "])\n",
    "\n",
    "# Preparing the training and test set\n",
    "trainset = torchvision.datasets.MNIST('mnist', train=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST('mnist', train=False, transform=transform)\n",
    "\n",
    "# Prepare loader\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=0)\n",
    "import torch.optim as optim\n",
    "\n",
    "net = Net()\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Compute the predictions from the net.\n",
    "Using the predictions and the labels, compute the loss function.\n",
    "Compute the gradients for each weight.\n",
    "Update the weights using the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the forward pass\n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    # Compute the loss function\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using CNNs to make predictions\n",
    "Building and training neural networks is a very exciting job (trust me, I do it every day)! However, the main utility of neural networks is to make predictions. This is the entire reason why the field of deep learning has bloomed in the last few years, as neural networks predictions are extremely accurate. On this exercise, we are going to use the convolutional neural network you already trained in order to make predictions on the MNIST dataset.\n",
    "\n",
    "Remember that torch.max() takes two arguments: -output.data - the tensor which contains the data.\n",
    "\n",
    "Either 1 to do argmax or 0 to do max\n",
    "\n",
    "Iterate over the given test_loader, saving the results of each iteration in data.\n",
    "Get the image and label from the data tuple, storing the results in image and label.\n",
    "Make a forward pass in the net using your image.\n",
    "Get the net prediction using torch.max() function.\n",
    "\n",
    "\n",
    "LETS SEE How accurate predictions are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yipes, your net made the right prediction tensor([7])\n",
      "Yipes, your net made the right prediction tensor([2])\n",
      "Yipes, your net made the right prediction tensor([1])\n",
      "Yipes, your net made the right prediction tensor([0])\n",
      "Yipes, your net made the right prediction tensor([4])\n",
      "Yipes, your net made the right prediction tensor([1])\n",
      "Yipes, your net made the right prediction tensor([4])\n",
      "Yipes, your net made the right prediction tensor([9])\n",
      "Yipes, your net made the right prediction tensor([5])\n",
      "Yipes, your net made the right prediction tensor([9])\n",
      "Yipes, your net made the right prediction tensor([0])\n",
      "Yipes, your net made the right prediction tensor([6])\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "# Iterate over the data in the test_loader\n",
    "for i, data in enumerate(test_loader):\n",
    "    # Get the image and label from data\n",
    "    image, label = data\n",
    "    \n",
    "    # Make a forward pass in the net with your image\n",
    "    output = net(image)\n",
    "    \n",
    "    # Argmax the results of the net\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    \n",
    "    if predicted == label:\n",
    "        print(\"Yipes, your net made the right prediction \" + str(predicted))\n",
    "    else:\n",
    "        print(\"Your net prediction was \" + str(predicted) + \", but the correct label is: \" + str(label))\n",
    "        \n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Convolutional Neural Networks\n",
    "In this last chapter, we learn how to make neural networks work well in practice, using concepts like regularization, batch-normalization and transfer learning.\n",
    "\n",
    "\n",
    "https://campus.datacamp.com/courses/introduction-to-deep-learning-with-pytorch/using-convolutional-neural-networks?ex=1\n",
    "\n",
    "##### The sequential module\n",
    "sequential module - PyTorch tool that allows to train neural networks in easier way\n",
    "\n",
    "\n",
    "###### AlexNet declaring the modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 == nn.Conv2d(3, 64, kernel_size=11, stride = 4, padding = 2)\n",
    "        self.relu == nn.ReLU(inplace=True)\n",
    "        self.maxpool == nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 == nn.Conv2d(64, 192, kernel_size=5, padding = 2)\n",
    "        self.conv3 == nn.Conv2d(192, 384, kernel_size=3, padding = 1)\n",
    "        self.conv4 == nn.Conv2d(384, 256, kernel_size=3, padding = 1)\n",
    "        self.conv5 == nn.Conv2d(256, 256, kernel_size=3, padding = 1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "    #apply each of these layers sequentially\n",
    "    def forward(self,x):\n",
    "        x=self.relu(self.conv1(x))\n",
    "        x=self.maxpool(x)\n",
    "        x=self.relu(self.conv2(x))\n",
    "        x=self.maxpool(x)\n",
    "        x=self.relu(self.conv3(x))\n",
    "        x=self.relu(self.conv4(x))\n",
    "        x=self.relu(self.conv5(x))\n",
    "        x=self.maxpool(x)\n",
    "        x=self.avgpool(x)\n",
    "        x=x.view(x.size(0), 256 * 6 * 6)\n",
    "        x=self.relu(self.fc1(x))\n",
    "        x=self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "net = AlexNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of code to write, but fortunately can write in sequential module, declaring the modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride = 4, padding = 2),nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding = 2),nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding = 1),nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding = 1),nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding = 1),nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n",
    "        #sequential module\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(), nn.Linear(256 * 6 * 6, 4096), nn.Relu(inplace=True),\n",
    "            nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes),)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        x=self.avgpool(x)\n",
    "        x=x.view(x.size(0), 256 * 6 * 6)\n",
    "        x=self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential module - init method\n",
    "Having learned about the sequential module, now is the time to see how you can convert a neural network that doesn't use sequential modules to one that uses them. We are giving the code to build the network in the usual way, and you are going to write the code for the same network using sequential modules.\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(7 * 7 * 40, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 10) \n",
    "We want the pooling layer to be used after the second and fourth convolutional layers, while the relu nonlinearity needs to be used after each layer except the last (fully-connected) layer. For the number of filters (kernels), stride, passing, number of channels and number of units, use the same numbers as above.\n",
    "\n",
    "Instructions\n",
    "Declare all the layers needed for feature extraction in the self.features.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Declare all the layers for feature extraction\n",
    "        self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=____, out_channels=____, kernel_size=____, padding=____), \n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(____=____, ____=____, ____=____, ____=____), \n",
    "                                      nn.ReLU(____=____),\n",
    "                                      nn.Conv2d(____=____, ____=____, ____=____, ____=____), \n",
    "                                      nn.MaxPool2d(____, ____), nn.ReLU(____=____))\n",
    "                                        # Declare all the layers for classification\n",
    "        self.classifier = nn.Sequential(nn.Linear(____ * ____ * ____, 1024), nn.ReLU(____=____),\n",
    "                                       \tnn.Linear(____, ____), nn.ReLU(____=____),\n",
    "                                        nn.Linear(____, ____))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Declare all the layers for feature extraction\n",
    "        self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1), \n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1), \n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1), \n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True))\n",
    "        # Declare all the layers for classification\n",
    "        self.classifier = nn.Sequential(nn.Linear(7 * 7 * 40, 1024), nn.ReLU(inplace=True),\n",
    "                                       \tnn.Linear(1024, 2048), nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(2048, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You're ready to implement the forward() method using the sequential modules you just wrote.\n",
    "\n",
    "##### sequentiall module - forward() method\n",
    "Now, that you have defined all the modules that the network needs, it is time to apply them in the forward() method. For context, we are giving the code for the forward() method, if the net was written in the usual way.\n",
    "Note: for evaluation purposes, the entire code of the class needs to be in the script. We are using the __init__ method as you have coded it on the previous exercise, while you are going to code the forward() method here.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Extract the features from the images.\n",
    "Squeeze the three spatial dimensions of the feature maps into one using the view() method.\n",
    "Classify images based on the extracted features.\n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Apply the feature extractor in the input\n",
    "        x = ____\n",
    "        \n",
    "        # Squeeze the three spatial dimensions in one\n",
    "        x = x.view(-1, ____ * ____ * ____)\n",
    "        \n",
    "        # Classify the images\n",
    "        x = ____\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x):\n",
    "      \n",
    "        # Apply the feature extractor in the input\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # Squeeze the three spatial dimensions in one\n",
    "        x = x.view(-1, 7 * 7 * 40)\n",
    "        \n",
    "        # Classify the images\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! You can see how the forward() method simplifies the whole process.\n",
    "\n",
    "#### The problem of overfitting\n",
    "\n",
    "How to prevent overfitting\n",
    "Training set\n",
    "Validation set\n",
    "Testing set\n",
    "\n",
    "Validation set:   cross-validation  ( no overlapping of datasets)\n",
    "Training set: train the model as before\n",
    "Validation set: select the model  (best model based on validation set)\n",
    "Testing set: test the model\n",
    "\n",
    "###### Creating validation sets in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arrange(50000)\n",
    "np.random.suffle(indices)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "                    transform=transforms.Compose([transoforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])),\n",
    "    #first 45000 for training\n",
    "    batch_size=1, suffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[:45000]))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "                transform=transforms.Compose([transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])),\n",
    "    #last 5000 for validation\n",
    "batch_size=1, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[45000:50000]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set\n",
    "You saw the need for validation set in the previous video. Problem is that the datasets typically are not separated into training, validation and testing. It is your job as a data scientist to split the dataset into training, testing and validation. The easiest (and most used) way of doing so is to do a random splitting of the dataset. In PyTorch, that can be done using SubsetRandomSampler object. You are going to split the training part of MNIST dataset into training and validation. After randomly shuffling the dataset, use the first 55000 points for training, and the remaining 5000 points for validation.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use numpy.arange() to create an array containing numbers [0, 59999] and then randomly shuffle the array.\n",
    "In the train_loader using SubsetRandomSampler() use the first 55k points for training.\n",
    "In the val_loader use the remaining 5k points for validation.\n",
    "\n",
    "# Shuffle the indices\n",
    "indices = ____\n",
    "np.____\n",
    "\n",
    "# Build the train loader\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
    "                     transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                     batch_size=64, shuffle=____, sampler=torch.utils.data.SubsetRandomSampler(indices[:____]))\n",
    "\n",
    "# Build the validation loader\n",
    "val_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
    "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                   batch_size=64, shuffle=____, sampler=____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "# Shuffle the indices\n",
    "indices = np.arange(60000)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Build the train loader\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
    "                     transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                     batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[:55000]))\n",
    "\n",
    "# Build the validation loader\n",
    "val_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
    "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                   batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[55000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting overfitting\n",
    "Overfitting is arguably the biggest problem in machine learning and data science, and being able to detect it will make you a much better data scientist. While reaching a high (or even perfect) accuracy on training sets is quite easy when you use neural networks, reaching a high accuracy on validation and testing sets is a very different thing.\n",
    "\n",
    "Let's see if you can now detect overfitting. Amongst the accuracy scores below, which network presents the biggest overfitting problem. ?\n",
    "\n",
    "\n",
    "The accuracy in the training set is 90%, the accuracy in the validation set is 70%.\n",
    "press\n",
    "\n",
    "Bravo, this is the correct answer! The accuracy in the training set is much higher than in the validation set, this is a typical example of overfitting.\n",
    "\n",
    "\n",
    "#### Regulatization techniques\n",
    "\n",
    "###### L2 - regularization\n",
    "\n",
    "#just add weight_decay order in optimizer\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4, weight_decay=0.0001)\n",
    "\n",
    "###### Dropout\n",
    "During the pass there is a probability of drop out at each computation\n",
    "Used in fully connected layers\n",
    "\n",
    "\n",
    "Can be used in AlexNet example\n",
    "self.classifier = nn.Sequential(\n",
    "#drop at each unit with probability of 50% p=0.5\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(256 * 6 * 6, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, num_classes),\n",
    "    )\n",
    "    \n",
    "###### Batch-normalization (for large neural networks)\n",
    "self.bn = nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.9)\n",
    "\n",
    "\n",
    "###### Early - stopping\n",
    "at the end of training we use the best performing network\n",
    "\n",
    "\n",
    "Hyperparameters, how to choose:\n",
    "l2 regulatization, dropout parameter, optimizers (Adam vs gradient descent), batch-norm momentum and epsilon, number of epochs for early stopping)\n",
    "\n",
    "Answer: Train many networks with different hyperparameters (typically use random values for them), and test them in the validation set. They use the best performing net in the validation set to know the expected accuracy of the network in new data\n",
    "\n",
    "Some regularization technique perform differently when net is getting trained or is getting validated\n",
    "eval() mode\n",
    "\n",
    "#sets the net in the train mode\n",
    "model.train()\n",
    "\n",
    "#sets the net in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "###### L2-regularization\n",
    "You are going to implement each of the regularization techniques explained in the previous video. Doing so, you will also remember important concepts studied throughout the course. You will start with l2-regularization, the most important regularization technique in machine learning. As you saw in the video, l2-regularization simply penalizes large weights, and thus enforces the network to use only small weights.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instantiate an object called model from class Net(), which is available in your workspace (consider it as a blackbox).\n",
    "Instantiate the cross-entropy loss.\n",
    "Instantiate Adam optimizer with learning_rate equals to 3e-4, and l2 regularization parameter equals to 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the network\n",
    "model = Net()\n",
    "\n",
    "# Instantiate the cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate the Adam optimizer with learning_rate equals # to 3e-4, and l2 regularization parameter equals to 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! See how easy it is to use l2 regularization in PyTorch? When you will start using bigger networks, this might well make the difference between your network overfitting or not.\n",
    "\n",
    "#### Dropout\n",
    "You saw that dropout is an effective technique to avoid overfitting. Typically, dropout is applied in fully-connected neural networks, or in the fully-connected layers of a convolutional neural network. You are now going to implement dropout and use it on a small fully-connected neural network.\n",
    "\n",
    "For the first hidden layer use 200 units, for the second hidden layer use 500 units, and for the output layer use 10 units (one for each class). For the activation function, use ReLU. Use .Dropout() with strength 0.5, between the first and second hidden layer. Use the sequential module, with the order being: fully-connected, activation, dropout, fully-connected, activation, fully-connected.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "2\n",
    "Implement the __init__ method, based on the description of the network in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Define all the parameters of the net\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #dropout betwenn first and second sequential layer\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(200, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(500, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Do the forward pass\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You now know how to efficiently regularize fully-connected layers.\n",
    "#### Batch-normalization\n",
    "Dropout is used to regularize fully-connected layers. Batch-normalization is used to make the training of convolutional neural networks more efficient, while at the same time having regularization effects. You are going to implement the __init__ method of a small convolutional neural network, with batch-normalization. The feature extraction part of the CNN will contain the following modules (in order): convolution, max-pool, activation, batch-norm, convolution, max-pool, relu, batch-norm.\n",
    "\n",
    "The first convolutional layer will contain 10 output channels, while the second will contain 20 output channels. As always, we are going to use MNIST dataset, with images having shape (28, 28) in grayscale format (1 channel). In all cases, the size of the filter should be 3, the stride should be 1 and the padding should be 1.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Implement the feature extraction part of the network, using the description in the context.\n",
    "Implement the fully-connected (classifier) part of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Implement the sequential module for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(10),\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(20)\n",
    "        )\n",
    "        \n",
    "        # Implement the fully connected layer for classification\n",
    "        self.fc = nn.Linear(in_features=20 * 7 * 7, out_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer learning\n",
    "\n",
    "\n",
    "###### Finetuning in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Net()\n",
    "\n",
    "#first load the old model\n",
    "model.load_state_dict(torch.load('cifar10_net_pth'))\n",
    "\n",
    "#change the number of units in the last layer = number of out channels\n",
    "model.fc = nn.Linear(4 * 4 * 1024, 100)\n",
    "# train and evaluate model, procedure is the same\n",
    "model.train()\n",
    "\n",
    "\n",
    "####### Freezing the layers\n",
    "#instantiate the model\n",
    "model= Net()\n",
    "\n",
    "#load the parameters from the old model\n",
    "model.load_state_dict(torch.load('cifar10_net.pth'))\n",
    "\n",
    "#interate over the parameters on the model\n",
    "#freeze all the layers bar the final one\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "#change the number of output units\n",
    "#in this case only final layer in updated\n",
    "model.fc = nn.Linear(4 * 4 * 1024, 100)\n",
    "\n",
    "#train and evaluate the model\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Torchvision\n",
    "Torchvision is a PyTorch library with many pretrained networks, ready to be used for your dataset\n",
    "For example you can use state of art models like resnet, by simply downloading them from torchvision with a single line of code as shown here.\n",
    "\n",
    "Than you just need to change number of units to number of classes in your dataset, and voila you have a very good net ready to be used without even needing to write the code for it.\n",
    "\n",
    "\n",
    "So, always consider finetuning an already pretrained net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchvision library\n",
    "import torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(512, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning a CNN\n",
    "Previously, you trained a model to classify handwritten digits and saved the model parameters to my_net.pth. Now you're going to classify handwritten letters, but you have a smaller training set.\n",
    "\n",
    "In the first step, you'll create a new model using this training set, but the accuracy will be poor. Next, you'll perform the same training, but you'll start with the parameters from your digit classifying model. Even though digits and letters are two different classification problems, you'll see that using information from your previous model will dramatically improve this one.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "2\n",
    "Create a new model using the Net() module.\n",
    "Change the number of output units, to the number of classifications for letters.\n",
    "\n",
    "#Create a new model\n",
    "model = ____\n",
    "\n",
    "#Change the number of out channels\n",
    "model.fc = nn.Linear(7 * 7 * 512, ____)\n",
    "\n",
    "#Train and evaluate the model\n",
    "model.train()\n",
    "train_net(model, optimizer, criterion)\n",
    "print(\"Accuracy of the net is: \" + str(model.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = Net()\n",
    "\n",
    "# Change the number of output units\n",
    "model.fc = nn.Linear(7 * 7 * 512, 26)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model.train()\n",
    "train_net(model, optimizer, criterion)\n",
    "print(\"Accuracy of the net is: \" + str(model.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torchvision module\n",
    "You already finetuned a net you had pretrained. In practice though, it is very common to finetune CNNs that someone else (typically the library's developers) have pretrained in ImageNet. Big networks still take a lot of time to be trained on large datasets, and maybe you cannot afford to train a large network on a dataset of 1.2 million images on your laptop.\n",
    "\n",
    "Instead, you can simply download the network and finetune it on your dataset. That's what you will do right now. You are going to assume that you have a personal dataset, containing the images from all your last 7 holidays. You want to build a neural network that can classify each image depending on the holiday it comes from. However, since the dataset is so small, you need to use the finetuning technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# Download resnet18\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the layers bar the last one\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Change the number of output units\n",
    "model.fc = nn.Linear(512, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finetuning a CNN\n",
    "Previously, you trained a model to classify handwritten digits and saved the model parameters to my_net.pth. Now you're going to classify handwritten letters, but you have a smaller training set.\n",
    "\n",
    "In the first step, you'll create a new model using this training set, but the accuracy will be poor. Next, you'll perform the same training, but you'll start with the parameters from your digit classifying model. Even though digits and letters are two different classification problems, you'll see that using information from your previous model will dramatically improve this one.\n",
    "\n",
    "Instructions 2/2\n",
    "50 XP\n",
    "2\n",
    "Repeat the training process, but first load the digit classifier parameters from my_net.pth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model using\n",
    "model = Net()\n",
    "\n",
    "# Load the parameters from the old model\n",
    "model.load_state_dict(torch.load('my_net.pth'))\n",
    "\n",
    "# Change the number of out channels\n",
    "model.fc = nn.Linear(7 * 7 * 512, 26)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model.train()\n",
    "train_net(model, optimizer, criterion)\n",
    "print(\"Accuracy of the net is: \" + str(model.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! By incorporating information from the previously trained model, we are able to get a good model for handwritten digits, even with a small training set!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
